{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a86f8df",
   "metadata": {},
   "source": [
    "# Ejercicio Práctico_Predicción en Streaming con Spark ML y Spark Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d06b6c8",
   "metadata": {},
   "source": [
    "En este notebook vamos a cargar un pipeline que tiene un conjunto de fases de pre-procesamiento y un modelo de clasificacion predecir la probabilidad de un paciente de sufrir un ataque al corazón. La predicción se realizará sobre datos en streaming optenidos a partir del csv de heart.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6318a507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f81f759",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56cecab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DieggsPapu:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[16]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Heart Disease</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x155363cbc10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "## Inicia una sesion de Spark\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[16]\")\\\n",
    "        .appName('Heart Disease')\\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8216cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType( \\\n",
    "                     [StructField(\"age\", LongType(),True), \\\n",
    "                      StructField(\"sex\", LongType(), True), \\\n",
    "                      StructField(\"cp\", LongType(), True), \\\n",
    "                      StructField('trestbps', LongType(), True), \\\n",
    "                      StructField(\"chol\", LongType(), True), \\\n",
    "                      StructField(\"fbs\", LongType(), True), \\\n",
    "                      StructField(\"restecg\", LongType(), True), \\\n",
    "                      StructField(\"thalach\", LongType(), True),\\\n",
    "                      StructField(\"exang\", LongType(), True), \\\n",
    "                      StructField(\"oldpeak\", DoubleType(), True), \\\n",
    "                      StructField(\"slope\", LongType(),True), \\\n",
    "                      StructField(\"ca\", LongType(), True), \\\n",
    "                      StructField(\"thal\", LongType(), True), \\\n",
    "                      StructField(\"target\", LongType(), True), \\\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afe3c31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "|age|sex| cp|trestbps|chol|fbs|restecg|thalach|exang|oldpeak|slope| ca|thal|target|\n",
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "| 63|  1|  3|     145| 233|  1|      0|    150|    0|    2.3|    0|  0|   1|     1|\n",
      "| 37|  1|  2|     130| 250|  0|      1|    187|    0|    3.5|    0|  0|   2|     1|\n",
      "| 41|  0|  1|     130| 204|  0|      0|    172|    0|    1.4|    2|  0|   2|     1|\n",
      "| 56|  1|  1|     120| 236|  0|      1|    178|    0|    0.8|    2|  0|   2|     1|\n",
      "| 57|  0|  0|     120| 354|  0|      1|    163|    1|    0.6|    2|  0|   2|     1|\n",
      "| 57|  1|  0|     140| 192|  0|      1|    148|    0|    0.4|    1|  0|   1|     1|\n",
      "| 56|  0|  1|     140| 294|  0|      0|    153|    0|    1.3|    1|  0|   2|     1|\n",
      "| 44|  1|  1|     120| 263|  0|      1|    173|    0|    0.0|    2|  0|   3|     1|\n",
      "| 52|  1|  2|     172| 199|  1|      1|    162|    0|    0.5|    2|  0|   3|     1|\n",
      "| 57|  1|  2|     150| 168|  0|      1|    174|    0|    1.6|    2|  0|   2|     1|\n",
      "| 54|  1|  0|     140| 239|  0|      1|    160|    0|    1.2|    2|  0|   2|     1|\n",
      "| 48|  0|  2|     130| 275|  0|      1|    139|    0|    0.2|    2|  0|   2|     1|\n",
      "| 49|  1|  1|     130| 266|  0|      1|    171|    0|    0.6|    2|  0|   2|     1|\n",
      "| 64|  1|  3|     110| 211|  0|      0|    144|    1|    1.8|    1|  0|   2|     1|\n",
      "| 58|  0|  3|     150| 283|  1|      0|    162|    0|    1.0|    2|  0|   2|     1|\n",
      "| 50|  0|  2|     120| 219|  0|      1|    158|    0|    1.6|    1|  0|   2|     1|\n",
      "| 58|  0|  2|     120| 340|  0|      1|    172|    0|    0.0|    2|  0|   2|     1|\n",
      "| 66|  0|  3|     150| 226|  0|      1|    114|    0|    2.6|    0|  0|   2|     1|\n",
      "| 43|  1|  0|     150| 247|  0|      1|    171|    0|    1.5|    2|  0|   2|     1|\n",
      "| 69|  0|  3|     140| 239|  0|      1|    151|    0|    1.8|    2|  2|   2|     1|\n",
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Carga y visualiza el csv de Ejercicios\\data\\heart.csv con el nombre de heart\n",
    "heart = spark.read.csv(\n",
    "    \"./data/heart.csv\",\n",
    "    sep = ',',\n",
    "    header = True,\n",
    "    schema = schema\n",
    "    )\n",
    "heart.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28d8812f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- sex: long (nullable = true)\n",
      " |-- cp: long (nullable = true)\n",
      " |-- trestbps: long (nullable = true)\n",
      " |-- chol: long (nullable = true)\n",
      " |-- fbs: long (nullable = true)\n",
      " |-- restecg: long (nullable = true)\n",
      " |-- thalach: long (nullable = true)\n",
      " |-- exang: long (nullable = true)\n",
      " |-- oldpeak: double (nullable = true)\n",
      " |-- slope: long (nullable = true)\n",
      " |-- ca: long (nullable = true)\n",
      " |-- thal: long (nullable = true)\n",
      " |-- label: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import StructType,StructField,LongType, StringType,DoubleType,TimestampType\n",
    "\n",
    "\n",
    "df = heart.withColumnRenamed(\"target\",\"label\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b2c3b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF, trainDF = df.randomSplit([0.3, 0.7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear el Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "088afddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "# Get the vectors, and make the stage 1, tranform\n",
    "stage_1 = VectorAssembler(inputCols=[\n",
    "    'age',\n",
    "    'sex',\n",
    "    'cp',\n",
    "    'trestbps',\n",
    "    'chol',\n",
    "    'fbs',\n",
    "    'restecg',\n",
    "    'thalach',\n",
    "    'exang',\n",
    "    'oldpeak',\n",
    "    'slope',\n",
    "    'ca',\n",
    "    'thal',\n",
    "    ], \n",
    "    outputCol='features'\n",
    "    )\n",
    "# Stage 2 perform a logistic regression, estimator\n",
    "stage_2 = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "# Regression pipeline, set up the pipeline\n",
    "regression_pipeline = Pipeline(stages=[stage_1, stage_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_pipeline1 = PipelineModel(stages=[stage_1, stage_2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling None.None. Trace:\npy4j.Py4JException: Cannot convert org.apache.spark.ml.classification.LogisticRegression to org.apache.spark.ml.Transformer\r\n\tat py4j.commands.ArrayCommand.convertArgument(ArrayCommand.java:166)\r\n\tat py4j.commands.ArrayCommand.setArray(ArrayCommand.java:144)\r\n\tat py4j.commands.ArrayCommand.execute(ArrayCommand.java:97)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\Cursos\\ApacheSpark\\Spark+Streaming1\\Spark Streaming\\Ejercicio_Predicción en Streaming con Spark ML y Spark Streaming.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Cursos/ApacheSpark/Spark%2BStreaming1/Spark%20Streaming/Ejercicio_Predicci%C3%B3n%20en%20Streaming%20con%20Spark%20ML%20y%20Spark%20Streaming.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m regression_pipeline1\u001b[39m.\u001b[39mwrite()\u001b[39m.\u001b[39moverwrite()\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39m./pipelines/sample2\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\pipeline.py:328\u001b[0m, in \u001b[0;36mPipelineModel.write\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    324\u001b[0m allStagesAreJava \u001b[39m=\u001b[39m PipelineSharedReadWrite\u001b[39m.\u001b[39mcheckStagesForJava(\n\u001b[0;32m    325\u001b[0m     cast(List[\u001b[39m\"\u001b[39m\u001b[39mPipelineStage\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstages)\n\u001b[0;32m    326\u001b[0m )\n\u001b[0;32m    327\u001b[0m \u001b[39mif\u001b[39;00m allStagesAreJava:\n\u001b[1;32m--> 328\u001b[0m     \u001b[39mreturn\u001b[39;00m JavaMLWriter(\u001b[39mself\u001b[39m)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    329\u001b[0m \u001b[39mreturn\u001b[39;00m PipelineModelWriter(\u001b[39mself\u001b[39m)\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\util.py:206\u001b[0m, in \u001b[0;36mJavaMLWriter.__init__\u001b[1;34m(self, instance)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, instance: \u001b[39m\"\u001b[39m\u001b[39mJavaMLWritable\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    205\u001b[0m     \u001b[39msuper\u001b[39m(JavaMLWriter, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m--> 206\u001b[0m     _java_obj \u001b[39m=\u001b[39m instance\u001b[39m.\u001b[39m_to_java()  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    207\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jwrite \u001b[39m=\u001b[39m _java_obj\u001b[39m.\u001b[39mwrite()\n",
      "File \u001b[1;32mC:\\spark\\python\\pyspark\\ml\\pipeline.py:363\u001b[0m, in \u001b[0;36mPipelineModel._to_java\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    361\u001b[0m java_stages \u001b[39m=\u001b[39m gateway\u001b[39m.\u001b[39mnew_array(\u001b[39mcls\u001b[39m, \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstages))\n\u001b[0;32m    362\u001b[0m \u001b[39mfor\u001b[39;00m idx, stage \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstages):\n\u001b[1;32m--> 363\u001b[0m     java_stages[idx] \u001b[39m=\u001b[39m cast(JavaParams, stage)\u001b[39m.\u001b[39m_to_java()\n\u001b[0;32m    365\u001b[0m _java_obj \u001b[39m=\u001b[39m JavaParams\u001b[39m.\u001b[39m_new_java_obj(\n\u001b[0;32m    366\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39morg.apache.spark.ml.PipelineModel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muid, java_stages\n\u001b[0;32m    367\u001b[0m )\n\u001b[0;32m    369\u001b[0m \u001b[39mreturn\u001b[39;00m _java_obj\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_collections.py:238\u001b[0m, in \u001b[0;36mJavaArray.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    235\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__repl_item_from_slice(self_range, value)\n\u001b[0;32m    237\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, \u001b[39mint\u001b[39m):\n\u001b[1;32m--> 238\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_item(key, value)\n\u001b[0;32m    239\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    240\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mlist indices must be integers, not \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    241\u001b[0m         key\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m))\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_collections.py:221\u001b[0m, in \u001b[0;36mJavaArray.__set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    219\u001b[0m command \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m    220\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m--> 221\u001b[0m \u001b[39mreturn\u001b[39;00m get_return_value(answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gateway_client)\n",
      "File \u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:330\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m         \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m             \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 330\u001b[0m         \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m             \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n\u001b[0;32m    333\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    334\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    335\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    336\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name))\n",
      "\u001b[1;31mPy4JError\u001b[0m: An error occurred while calling None.None. Trace:\npy4j.Py4JException: Cannot convert org.apache.spark.ml.classification.LogisticRegression to org.apache.spark.ml.Transformer\r\n\tat py4j.commands.ArrayCommand.convertArgument(ArrayCommand.java:166)\r\n\tat py4j.commands.ArrayCommand.setArray(ArrayCommand.java:144)\r\n\tat py4j.commands.ArrayCommand.execute(ArrayCommand.java:97)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n\n"
     ]
    }
   ],
   "source": [
    "regression_pipeline1.write().overwrite().save(\"./pipelines/sample2\")\n",
    "# regression_pipeline.write().overwrite().save(\"./pipelines/sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correr el pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the pipeline for the trainning data\n",
    "model = regression_pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the data\n",
    "sample_data_train = model.transform(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+-----+--------------------+--------------------+--------------------+----------+\n",
      "|age|sex| cp|trestbps|chol|fbs|restecg|thalach|exang|oldpeak|slope| ca|thal|label|            features|       rawPrediction|         probability|prediction|\n",
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+-----+--------------------+--------------------+--------------------+----------+\n",
      "| 29|  1|  1|     130| 204|  0|      0|    202|    0|    0.0|    2|  0|   2|    1|[29.0,1.0,1.0,130...|[-2.0716685397098...|[0.11188113879827...|       1.0|\n",
      "| 34|  1|  3|     118| 182|  0|      0|    174|    0|    0.0|    2|  0|   2|    1|[34.0,1.0,3.0,118...|[-3.4284364850358...|[0.03141847728676...|       1.0|\n",
      "| 35|  0|  0|     138| 183|  0|      1|    182|    0|    1.4|    2|  0|   2|    1|[35.0,0.0,0.0,138...|[-2.5265459825127...|[0.07401803472415...|       1.0|\n",
      "| 35|  1|  0|     120| 198|  0|      1|    130|    1|    1.6|    1|  0|   3|    0|[35.0,1.0,0.0,120...|[1.95000444956793...|[0.87544712699185...|       0.0|\n",
      "| 37|  0|  2|     120| 215|  0|      1|    170|    0|    0.0|    2|  0|   2|    1|[37.0,0.0,2.0,120...|[-4.5393834279591...|[0.01056713264108...|       1.0|\n",
      "| 37|  1|  2|     130| 250|  0|      1|    187|    0|    3.5|    0|  0|   2|    1|[37.0,1.0,2.0,130...|[-0.6637097800945...|[0.33990675393492...|       1.0|\n",
      "| 39|  0|  2|      94| 199|  0|      1|    179|    0|    0.0|    2|  0|   2|    1|[39.0,0.0,2.0,94....|[-5.3876665879903...|[0.00455181695025...|       1.0|\n",
      "| 39|  0|  2|     138| 220|  0|      1|    152|    0|    0.0|    1|  0|   2|    1|[39.0,0.0,2.0,138...|[-3.3303988382464...|[0.03454292667573...|       1.0|\n",
      "| 39|  1|  0|     118| 219|  0|      1|    140|    0|    1.2|    1|  0|   3|    0|[39.0,1.0,0.0,118...|[0.32935785195042...|[0.58160312413138...|       0.0|\n",
      "| 40|  1|  0|     110| 167|  0|      0|    114|    1|    2.0|    1|  0|   3|    0|[40.0,1.0,0.0,110...|[2.48057362030113...|[0.92276868776794...|       0.0|\n",
      "| 40|  1|  0|     152| 223|  0|      1|    181|    0|    0.0|    2|  0|   3|    0|[40.0,1.0,0.0,152...|[-0.6316986875030...|[0.34712546541193...|       1.0|\n",
      "| 41|  0|  1|     126| 306|  0|      1|    163|    0|    0.0|    2|  0|   2|    1|[41.0,0.0,1.0,126...|[-3.1504753128416...|[0.04107255363538...|       1.0|\n",
      "| 41|  0|  1|     130| 204|  0|      0|    172|    0|    1.4|    2|  0|   2|    1|[41.0,0.0,1.0,130...|[-2.7060863066614...|[0.06261517085811...|       1.0|\n",
      "| 41|  0|  2|     112| 268|  0|      0|    172|    1|    0.0|    2|  0|   2|    1|[41.0,0.0,2.0,112...|[-2.7537592476542...|[0.05987469275656...|       1.0|\n",
      "| 41|  1|  0|     110| 172|  0|      0|    158|    0|    0.0|    2|  0|   3|    0|(13,[0,1,3,4,7,10...|[-0.8574344544415...|[0.29787564019308...|       1.0|\n",
      "| 41|  1|  1|     135| 203|  0|      1|    132|    0|    0.0|    1|  0|   1|    1|[41.0,1.0,1.0,135...|[-1.2059976883777...|[0.23040997986849...|       1.0|\n",
      "| 41|  1|  2|     130| 214|  0|      0|    168|    0|    2.0|    1|  0|   2|    1|[41.0,1.0,2.0,130...|[-1.0441388080331...|[0.26035219791726...|       1.0|\n",
      "| 42|  0|  0|     102| 265|  0|      0|    122|    0|    0.6|    1|  0|   2|    1|(13,[0,3,4,7,9,10...|[-1.1618898741046...|[0.23832405480076...|       1.0|\n",
      "| 42|  1|  0|     136| 315|  0|      1|    125|    1|    1.8|    1|  0|   1|    0|[42.0,1.0,0.0,136...|[2.17029442797561...|[0.89755004354420...|       0.0|\n",
      "| 42|  1|  0|     140| 226|  0|      1|    178|    0|    0.0|    2|  0|   2|    1|[42.0,1.0,0.0,140...|[-1.2380502816260...|[0.22477554476229...|       1.0|\n",
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# output\n",
    "sample_data_train.select(\"*\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32me:\\Cursos\\ApacheSpark\\Spark+Streaming1\\Spark Streaming\\Ejercicio_Predicción en Streaming con Spark ML y Spark Streaming.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Cursos/ApacheSpark/Spark%2BStreaming1/Spark%20Streaming/Ejercicio_Predicci%C3%B3n%20en%20Streaming%20con%20Spark%20ML%20y%20Spark%20Streaming.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39mwrite\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39m./pipelines/samplemodel\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "model.write.save(\"./pipelines/samplemodel\")\n",
    "# model_in = PipelineModel.load(outpath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a8d0d8",
   "metadata": {},
   "source": [
    "### Carga del Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7be455",
   "metadata": {},
   "outputs": [],
   "source": [
    "pModel = PipelineModel.load(\"\\pipelines\")\n",
    "## Comprueba que el pipeline anterior funciona correctamente. Para ello realiza una prediccion sobre el conjunto de \n",
    "## datos de trainDF y muestra la prediccion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2077daa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testData = testDF.repartition(10)\n",
    "\n",
    "testData.write.format(\"CSV\").option(\"header\",False).save(\"/heart_streaming/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984bbf3e",
   "metadata": {},
   "source": [
    "## Creando predicciones en Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8489bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utiliza los csv guardados en data/heart_streaming para simular un proceso de datos en streaming.\n",
    "## Para ello, utiliza la funcion de spark.readStream \n",
    "## En la configuración pon: que se importe un archivo por ejecucion\n",
    "## que se renombre la variable de \"output\"a \"label\"\n",
    "## Llama a este proceso con el nombre sourceStream\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c9160b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utiliza el pipeline \"pModel\" para realizar predicciones utilizando los datos en streaming de \"sourceStream\"\n",
    "## De la predicción selecciona las variables label, probability, prediction. \n",
    "## Llama a este proceso con el nombre de \"prediction1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6113aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(prediction1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50112be",
   "metadata": {},
   "source": [
    "#### Mostrando las predicciones en consola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836cdda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Obten las predicciones sobre los datos en streaming, para ello utiliza prediction1.writeStream. En las opciones de\n",
    "## configuracion pon: \"format\" igual a \"console\" \n",
    "## en .trigger igual (once=True),\n",
    "## y permite que el proceso espere hasta que se complete con .awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d638e3",
   "metadata": {},
   "source": [
    "#### Guardando las perdicciones en Memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2672a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Obten las predicciones sobre los datos en streaming, para ello utiliza prediction1.writeStream. \n",
    "## En las opciones de configuracion pon: que los resultados se guarden en memoria, \n",
    "## que el .outputMode sea \"append\"\n",
    "## que el nombre de la query \"queryName\" sea \"prediction4\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34147a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(2):\n",
    "    df = spark.sql(\n",
    "        \"SELECT * FROM prediction4\")\n",
    "    df.show(10)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9940d9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Valida que el proceso de streaming está activo y después muestra el estado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3391605d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
